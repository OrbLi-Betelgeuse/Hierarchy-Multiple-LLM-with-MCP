#!/usr/bin/env python3
"""
Simple Test for Manager-Executor Collaboration System

Basic functionality test without LLM calls to avoid hanging.
"""

import asyncio
import logging
from models.mcp_protocol import MCPProtocol, MCPManager, MCPExecutor
from models.llm_interface import create_llm_interface
from models.manager import Manager
from models.executor import Executor

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)


async def test_mcp_protocol():
    """Test MCP protocol functionality."""
    print("ğŸ§ª Testing MCP Protocol...")

    try:
        protocol = MCPProtocol()

        # Test message creation
        message = protocol.create_task_decomposition_message(
            "manager_01", "executor_01", "Test task", "simple"
        )
        print("âœ… MCP message created successfully")

        # Test message serialization
        serialized = protocol.serialize_message(message)
        print("âœ… MCP message serialized successfully")

        # Test message parsing
        parsed = protocol.parse_message(serialized)
        print("âœ… MCP message parsed successfully")

        return True
    except Exception as e:
        print(f"âŒ MCP protocol test failed: {e}")
        return False


async def test_manager_executor_setup():
    """Test Manager and Executor setup."""
    print("\nğŸ§ª Testing Manager-Executor Setup...")

    try:
        # Create MCP Manager and Executor
        manager = MCPManager("test_manager")
        executor = MCPExecutor("test_executor", ["general"])

        print("âœ… MCP Manager and Executor created successfully")

        # Test task decomposition
        available_executors = ["executor_01", "executor_02"]
        tasks = await manager.decompose_task(
            "Test task description", available_executors
        )
        print(f"âœ… Task decomposition created {len(tasks)} subtasks")

        return True
    except Exception as e:
        print(f"âŒ Manager-Executor setup test failed: {e}")
        return False


async def test_llm_interface_creation():
    """Test LLM interface creation without making calls."""
    print("\nğŸ§ª Testing LLM Interface Creation...")

    try:
        # Test Ollama interface creation
        llm = create_llm_interface("ollama", "llama2:7b")
        print("âœ… LLM interface created successfully")

        # Test OpenAI interface creation (if API key available)
        try:
            llm_openai = create_llm_interface("openai", "gpt-3.5-turbo")
            print("âœ… OpenAI interface created successfully")
        except Exception as e:
            print(f"âš ï¸ OpenAI interface creation failed (expected): {e}")

        return True
    except Exception as e:
        print(f"âŒ LLM interface creation test failed: {e}")
        return False


async def test_basic_workflow():
    """Test basic workflow without LLM calls."""
    print("\nğŸ§ª Testing Basic Workflow...")

    try:
        # Create manager and executor
        manager = MCPManager("test_manager")
        executor = MCPExecutor("test_executor", ["summarization", "general"])

        # Register executor
        manager.protocol.register_executor(
            "test_executor", ["summarization", "general"]
        )

        # Create a simple task
        task = manager.protocol.task_registry.get("test_task")
        if not task:
            # Create a test task
            task = manager.protocol.task_registry["test_task"] = {
                "task_id": "test_task",
                "description": "Test task",
                "status": "pending",
            }

        print("âœ… Basic workflow setup completed")
        return True
    except Exception as e:
        print(f"âŒ Basic workflow test failed: {e}")
        return False


async def test_basic_functionality():
    """Test basic Manager-Executor functionality."""
    print("ğŸš€ Starting basic functionality test...")

    try:
        # Create LLM interfaces
        print("ğŸ“ Creating LLM interfaces...")
        manager_llm = create_llm_interface(
            provider="ollama",
            model_name="llama2:7b",
            base_url="http://localhost:11434",
        )
        executor_llm = create_llm_interface(
            provider="ollama",
            model_name="llama2:7b",
            base_url="http://localhost:11434",
        )

        # Create manager and executor
        print("ğŸ‘¨â€ğŸ’¼ Creating manager...")
        manager = Manager(manager_id="test_manager", llm_interface=manager_llm)

        print("ğŸ‘· Creating executor...")
        executor = Executor(
            executor_id="test_executor",
            llm_interface=executor_llm,
            capabilities=["summarization", "general"],
        )

        # Register executor
        print("ğŸ“‹ Registering executor...")
        await manager.register_executor(
            executor_id="test_executor", capabilities=["summarization", "general"]
        )

        # Start executor
        print("â–¶ï¸ Starting executor...")
        await executor.start()

        # Test simple task execution
        print("ğŸ¯ Testing task execution...")
        test_task_description = "Summarize this text: Artificial intelligence is transforming healthcare through improved diagnostics and patient care."

        print("â³ Executing task (this may take a while for first run)...")
        result = await manager.execute_task(test_task_description, "summarization")

        print("âœ… Task execution completed!")
        print(f"ğŸ“Š Result: {result}")

        # Stop executor
        print("â¹ï¸ Stopping executor...")
        await executor.stop()

        print("ğŸ‰ All tests passed!")

    except Exception as e:
        print(f"âŒ Test failed: {e}")
        logger.error(f"Test error: {e}", exc_info=True)
        raise


async def main():
    """Run all tests."""
    print("ğŸš€ Starting Simple Manager-Executor Collaboration System Tests\n")

    tests = [
        ("MCP Protocol", test_mcp_protocol),
        ("Manager-Executor Setup", test_manager_executor_setup),
        ("LLM Interface Creation", test_llm_interface_creation),
        ("Basic Workflow", test_basic_workflow),
        ("Basic Functionality", test_basic_functionality),
    ]

    results = []

    for test_name, test_func in tests:
        try:
            print(f"\n{'='*20} {test_name} {'='*20}")
            result = await test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"âŒ {test_name} test crashed: {e}")
            results.append((test_name, False))

    # Print summary
    print("\n" + "=" * 50)
    print("ğŸ“Š Test Results Summary")
    print("=" * 50)

    passed = 0
    for test_name, result in results:
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"{test_name}: {status}")
        if result:
            passed += 1

    print(f"\nOverall: {passed}/{len(results)} tests passed")

    if passed == len(results):
        print("ğŸ‰ All tests passed! System is ready for experiments.")
    elif passed >= len(results) - 1:
        print("âœ… Most tests passed! System should work for experiments.")
    else:
        print("âš ï¸ Some tests failed. Please check the setup.")

    print("\nğŸ’¡ To run full experiments with LLM calls:")
    print("   python pipeline.py --experiment summarization")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nâš ï¸ Test interrupted by user")
    except Exception as e:
        print(f"\nâŒ Test failed with error: {e}")
