"""
è¯·æŠŠè¿™ä¸ªpyæ–‡ä»¶æ”¾åœ¨å’Œpipeline.pyç›¸åŒçš„è·¯å¾„ä¸‹
è¿™ä¸ªTask_B_Pipelineæ˜¯æ ¹æ®pipeline.pyä¿®æ”¹çš„ï¼Œé’ˆå¯¹äºTaskBçš„å˜ç§ä»£ç 
ä¿®æ”¹çš„å¤§éƒ¨åˆ†åœ¨run_task_b_pipeline()ä¸­
ä»£ç æ•´ä½“å­˜åœ¨æ ‡çº¢çš„éƒ¨åˆ†ï¼Œä½†æ˜¯ä¸å½±å“æµ‹è¯•è¿è¡Œ
ä»£ç æ€è·¯ä¸ºï¼šæ‹¿åˆ°æå‡ºçš„é—®é¢˜ï¼ˆTODO:éœ€è¦æ”¹æˆä»æ–‡ä»¶è¯»å…¥ï¼Œæ•°æ®é›†å’ŒSummaryå®éªŒçš„æœ€å¥½ä¸€è‡´ï¼‰
ä¹‹åä¸¢ç»™Pudgeæ¨¡å‹åˆ‡ç‰‡æˆä¸‰ä¸ªä¸åŒçš„å°é—®é¢˜ï¼Œä¼ ç»™å­å¤„ç†æ¨¡å‹ï¼ˆsub_executorsï¼‰
æœ€ç»ˆé‡ç”¨Pudgeæ¥å¯¹ç»“æœè¿›è¡Œæ•´åˆå¹¶è¾“å‡º

ä½¿ç”¨çš„è¯„ä»·æŒ‡æ ‡ä¸ºä¹‹å‰å†™å¥½çš„utils/evaluation.pyï¼Œè¿è¡Œç»“æœå¤§éƒ¨åˆ†ä¿å­˜åœ¨results/comprehensive_report.jsonä¸­ï¼ˆTODOï¼šå‰©ä¸‹çš„åŠ¨æ€route resultså’Œmetricsä¸çŸ¥é“æœ‰ä»€ä¹ˆå¿…è¦ï¼Œæš‚æ—¶è¿˜æ²¡å†™ï¼‰

å°å‹å®éªŒä»£ç åœ¨pudge_experiment.py task_solver_experiment.py å’Œ test.pyä¸­ï¼Œå¯å…·ä½“æŸ¥çœ‹
TODOï¼šéœ€è¦æŸ¥çœ‹evaluateä¸­çš„expected resultï¼Œæˆ‘å¥½åƒæ²¡æœ‰æ‰¾åˆ°æ­£ç¡®çš„æ¯”å¯¹æ–¹æ³•ã€‚å…·ä½“å¯ä»¥æŸ¥çœ‹experiments/summarization_experiment.pyçš„æ¯”å¯¹æ–¹æ³•ï¼Œè®©GPTè¿ç§»åˆ°QAé—®é¢˜ä¸Šï¼ŒæŸ¥çœ‹å…·ä½“çš„ç»“æœã€‚
"""

import asyncio
import json
import logging
import argparse
import os
from typing import Dict, List, Any, Optional
from pathlib import Path
import click
from rich.console import Console
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.panel import Panel

# Import our modules
from models.manager import Manager
from models.executor import Executor, SpecializedExecutor
from models.llm_interface import create_llm_interface
from experiments.summarization_experiment import SummarizationExperiment
from experiments.qa_experiment import QAExperiment
from experiments.table_generation_experiment import TableGenerationExperiment
from utils.evaluation import Evaluator, PerformanceMonitor
from pprint import pprint
import time

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[logging.FileHandler("experiment.log"), logging.StreamHandler()],
)
logger = logging.getLogger(__name__)

console = Console()


class ExperimentPipeline:
    """Main pipeline for running Manager-Executor collaboration experiments."""

    def __init__(self, config_path: Optional[str] = None):
        self.config = self._load_config(config_path)
        self.evaluator = Evaluator()
        self.performance_monitor = PerformanceMonitor()
        self.results = {}

    def _load_config(self, config_path: Optional[str]) -> Dict[str, Any]:
        """Load configuration from file or use defaults."""
        if config_path and os.path.exists(config_path):
            with open(config_path, "r") as f:
                return json.load(f)

        # Default configuration
        return {
            "manager": {
              "provider": "ollama",
              "model": "deepseek-r1:7b",
              "kwargs": {"base_url": "http://localhost:11435"}
            },
            "executors": [
                {
                    "provider": "ollama",
                    "model": "deepseek-r1:1.5b",
                    "executor_id": "executor1",
                    "kwargs": {"base_url": "http://localhost:11434"}
                },
                {
                    "provider": "ollama",
                    "model": "deepseek-r1:1.5b",
                    "executor_id": "executor2",
                    "kwargs": {"base_url": "http://localhost:11436"}
                # },
                # {
                #     "provider": "ollama",
                #     "model": "qwen2.5:14b",
                #     "executor_id": "executor2",
                #     "kwargs": {"base_url": "http://localhost:11434"}
                # },
                # {
                #     "provider": "ollama",
                #     "model": "qwen2.5:14b",
                #     "executor_id": "executor3",
                #     "kwargs": {"base_url": "http://localhost:11434"}
                }
            ],
            "experiments": {
                "summarization": {"enabled": False, "num_tasks": 2},
                "question_answering": {"enabled": True, "num_tasks": 2},
                "table_generation": {"enabled": False, "num_tasks": 2},
            },
            "output": {
                "results_dir": "results",
                "export_metrics": True,
                "generate_visualizations": False,
            },
        }

    async def setup_system(self):
        """Setup the Manager-Executor system."""
        console.print(
            Panel.fit("Setting up Manager-Executor System", style="bold blue")
        )

        try:
            # Create manager
            manager_config = self.config["manager"]
            manager_llm = create_llm_interface(
                provider=manager_config["provider"],
                model_name=manager_config["model"],
                **manager_config.get("kwargs", {}),
            )

            self.manager = Manager(
                manager_id=manager_config["manager_id"], llm_interface=manager_llm
            )

            # Create executors
            self.executors = []
            for config in self.config["executors"]:
                executor_llm = create_llm_interface(
                    provider=config["provider"],
                    model_name=config["model"],
                    **config.get("kwargs", {}),
                )

                if config.get("specialized", False):
                    executor = SpecializedExecutor(
                        executor_id=config["executor_id"],
                        llm_interface=executor_llm,
                        task_type=config.get("task_type", "general"),
                        specialized_capabilities=config["capabilities"],
                    )
                else:
                    executor = Executor(
                        executor_id=config["executor_id"],
                        llm_interface=executor_llm,
                        capabilities=config["capabilities"],
                    )

                self.executors.append(executor)

                # Register executor with manager
                await self.manager.register_executor(
                    executor_id=config["executor_id"],
                    capabilities=config["capabilities"],
                )

            # Store executor references in manager for direct access
            self.manager.executor_instances = {
                executor.executor_id: executor for executor in self.executors
            }

            console.print(
                f"âœ… System setup complete: 1 manager, {len(self.executors)} executors"
            )

        except Exception as e:
            console.print(f"âŒ Error setting up system: {e}", style="bold red")
            raise

    async def run_task_b_pipeline(self, input_text: str) -> str:
        """è¿è¡ŒåŸºäº TaskB æ¶æ„çš„ä»»åŠ¡å¤„ç†æµç¨‹"""
        self.performance_monitor.start_monitoring()  # âœ… å¼€å§‹ç›‘æ§
        try:
            # åˆå§‹åŒ– pudge æ¨¡å‹ä¸º decomposer
            decomposer_cfg = self.config["decomposer"]
            pudge_llm = create_llm_interface(
                provider=decomposer_cfg["provider"],
                model_name=decomposer_cfg["model"],
                **decomposer_cfg.get("kwargs", {})
            )

            # åˆå§‹åŒ–ä¸‰ä¸ª executor æ¨¡å‹ï¼ˆ20Bï¼‰
            sub_executors = []
            for executor_cfg in self.config["executors"]:
                llm = create_llm_interface(
                    provider=executor_cfg["provider"],
                    model_name=executor_cfg["model"],
                    **executor_cfg.get("kwargs", {})
                )
                sub_executors.append(Executor(
                    executor_id=executor_cfg["executor_id"],
                    llm_interface=llm,
                    capabilities=[]
                ))

            # æ­¥éª¤1ï¼šç”¨ pudge åˆ†è§£ä»»åŠ¡
            prompt = (
                f"è¯·å°†ä¸‹é¢è¿™ä¸ªé—®é¢˜æ‹†åˆ†æˆä¸‰ä¸ªç‹¬ç«‹çš„æ­¥éª¤ï¼Œåªè¿”å›ä¸€ä¸ª JSON æ•°ç»„ï¼Œä¾‹å¦‚ï¼š"
                f'["æ­¥éª¤1", "æ­¥éª¤2", "æ­¥éª¤3"]ã€‚\n\nä»»åŠ¡å¦‚ä¸‹ï¼š{input_text}'
            )

            response = await pudge_llm.generate(prompt)

            # æå–å­—ç¬¦ä¸²å†…å®¹
            if isinstance(response, dict):
                content = response.get("content", "")
            else:
                content = getattr(response, "content", str(response))

            # æ‰“å°å†…å®¹è°ƒè¯•ï¼ˆæ¨èä¿ç•™ï¼‰
            print("ğŸ§ª Pudge è¿”å›å†…å®¹å¦‚ä¸‹ â†“â†“â†“")
            print(content)
            print("â›“ï¸ å°è¯•å°†å…¶è§£æä¸º JSON...")

            # åˆ¤æ–­æ˜¯å¦ä¸ºç©ºæˆ–ç©ºæ ¼
            if not isinstance(content, str) or not content.strip():
                raise ValueError("âŒ Pudge æ¨¡å‹è¿”å›ä¸ºç©ºï¼Œæ— æ³•è§£æä¸ºæ­¥éª¤")

            # å°è¯•è§£æä¸º JSON
            try:
                steps = json.loads(content)
            except json.JSONDecodeError as e:
                raise ValueError(f"âŒ JSON è§£æå¤±è´¥ï¼Œæ¨¡å‹è¿”å›å†…å®¹å¦‚ä¸‹ï¼š\n{content}") from e

            # æ‰“å°æŸ¥çœ‹æ˜¯å¦è¿”å›äº†å†…å®¹
            if not content.strip():
                raise ValueError("Pudge æ¨¡å‹è¿”å›ä¸ºç©ºï¼Œæ— æ³•è§£æä¸ºæ­¥éª¤")

            try:
                steps = json.loads(content)
            except json.JSONDecodeError as e:
                print(f"âŒ JSON è§£æå¤±è´¥ï¼Œæ¨¡å‹è¿”å›å†…å®¹ä¸ºï¼š{content}")
                raise e

            # æ­¥éª¤2ï¼šä¸‰ä¸ªæ¨¡å‹å¹¶è¡Œå¤„ç†å„è‡ªæ­¥éª¤
            sub_results = await asyncio.gather(*[
                executor.run_step(steps[i]) for i, executor in enumerate(sub_executors)
            ])

            # æ­¥éª¤3ï¼šå°†ç»“æœäº¤è¿˜ç»™ pudge è¿›è¡Œæ•´åˆ
            merge_prompt = f"è¯·æ ¹æ®ä»¥ä¸‹ä¸‰ä¸ªç»“æœæ•´åˆæˆä¸€ä¸ªå®Œæ•´ç­”æ¡ˆï¼š{json.dumps(sub_results, ensure_ascii=False)}"
            print(f"è¯·æ ¹æ®ä»¥ä¸‹ä¸‰ä¸ªç»“æœæ•´åˆæˆä¸€ä¸ªå®Œæ•´ç­”æ¡ˆï¼š{json.dumps(sub_results, ensure_ascii=False)}")
            final_response = await pudge_llm.generate(merge_prompt)

            # å–è¿”å›å†…å®¹
            if isinstance(final_response, dict):
                final_response_text = final_response.get("content", "")
            else:
                final_response_text = getattr(final_response, "content", str(final_response))

            # æ‰“å°ç»“æœ
            print(final_response_text)

            # âœ… è®°å½•æ€»è€—æ—¶
            execution_time = time.time() - self.performance_monitor.start_time

            # âœ… æå–æ¨¡å‹æœ€ç»ˆç»“æœï¼ˆå…¼å®¹ä¸åŒæ ¼å¼ï¼‰
            final_output = getattr(final_response, "content", None)
            if not final_output:
                final_output = final_response if isinstance(final_response, str) else str(final_response)

            # âœ… æ„é€  evaluation èƒ½è¯†åˆ«çš„ç»“æœç»“æ„
            self.results["taskb"] = {
                "experiment_type": "taskb",
                "total_tasks": 1,
                "successful_tasks": 1,
                "execution_times": [execution_time],
                "detailed_results": [
                    {
                        "status": "completed",
                        "execution_time": execution_time,
                        "output": final_output
                    }
                ],
                "manager_metrics": {
                    "model": self.config.get("decomposer", {}).get("model", "unknown"),
                    "task": "task decomposition and final synthesis"
                },
                "executor_metrics": {
                    executor.executor_id: {
                        "model": executor.llm_interface.model_name if hasattr(executor.llm_interface,
                                                                              'model_name') else "unknown",
                        "task": f"subtask-{i + 1}"
                    }
                    for i, executor in enumerate(sub_executors)
                }
            }

            # âœ… ç”Ÿæˆç»¼åˆæŠ¥å‘Š
            report = self.generate_comprehensive_report()

            # âœ… å±•ç¤ºç»“æœ
            self.display_results(report)

            # âœ… å¯¼å‡ºç»“æœ
            self.export_results(report)

            # âœ… è¿”å›ç»“æœï¼ˆç”¨äº CLI æ˜¾ç¤ºç­‰ç”¨é€”ï¼‰
            return final_output

        except Exception as e:
            logger.error(f"TaskB å¤±è´¥: {e}", exc_info=True)
            raise

        finally:
            self.performance_monitor.stop_monitoring()  # âœ… åœæ­¢ç›‘æ§

    async def run_single_experiment(self, experiment_type: str) -> Dict[str, Any]:
        """Run a single experiment."""
        await self.setup_system()
        console.print(
            Panel.fit(
                f"Running {experiment_type.title()} Experiment", style="bold magenta"
            )
        )

        self.performance_monitor.start_monitoring()

        try:
            if experiment_type == "summarization":
                result = await self.run_summarization_experiment()
            elif experiment_type == "qa":
                result = await self.run_qa_experiment()
            elif experiment_type == "table":
                result = await self.run_table_generation_experiment()
            else:
                raise ValueError(f"Unknown experiment type: {experiment_type}")

            self.results[experiment_type] = result

            # Generate report
            report = self.generate_comprehensive_report()

            # Display results
            self.display_results(report)

            # Export results
            self.export_results(report)

            console.print(
                Panel.fit(
                    f"{experiment_type.title()} Experiment Completed Successfully! ğŸ‰",
                    style="bold green",
                )
            )

            return result

        except Exception as e:
            console.print(
                f"âŒ {experiment_type} experiment failed: {e}", style="bold red"
            )
            logger.error(f"{experiment_type} experiment failed: {e}")
            raise
        finally:
            self.performance_monitor.stop_monitoring()

    # async def run_all_experiments(self) -> Dict[str, Any]:
    #     """Run all enabled experiments."""
    #     console.print(Panel.fit("Starting All Experiments", style="bold magenta"))

    #     self.performance_monitor.start_monitoring()

    #     all_results = {}

    #     with Progress(
    #         SpinnerColumn(),
    #         TextColumn("[progress.description]{task.description}"),
    #         console=console,
    #     ) as progress:

    #         # Summarization experiment
    #         if experiments_config["summarization"]["enabled"]:
    #             task = progress.add_task(
    #                 "Running summarization experiment...", total=None
    #             )
    #             try:
    #                 result = await self.run_summarization_experiment()
    #                 all_results["summarization"] = result
    #                 progress.update(task, description="âœ… Summarization completed")
    #             except Exception as e:
    #                 progress.update(task, description=f"âŒ Summarization failed: {e}")
    #                 logger.error(f"Summarization experiment failed: {e}")

    #         # Question answering experiment
    #         if experiments_config["question_answering"]["enabled"]:
    #             task = progress.add_task("Running QA experiment...", total=None)
    #             try:
    #                 result = await self.run_qa_experiment()
    #                 all_results["question_answering"] = result
    #                 progress.update(task, description="âœ… QA completed")
    #             except Exception as e:
    #                 progress.update(task, description=f"âŒ QA failed: {e}")
    #                 logger.error(f"QA experiment failed: {e}")

    #         # Table generation experiment
    #         if experiments_config["table_generation"]["enabled"]:
    #             task = progress.add_task(
    #                 "Running table generation experiment...", total=None
    #             )
    #             try:
    #                 result = await self.run_table_generation_experiment()
    #                 all_results["table_generation"] = result
    #                 progress.update(task, description="âœ… Table generation completed")
    #             except Exception as e:
    #                 progress.update(
    #                     task, description=f"âŒ Table generation failed: {e}"
    #                 )
    #                 logger.error(f"Table generation experiment failed: {e}")

    #     self.performance_monitor.stop_monitoring()
    #     self.results = all_results

    #     return all_results

    def generate_comprehensive_report(self) -> Dict[str, Any]:
        """Generate a comprehensive report of all experiments."""
        console.print(Panel.fit("Generating Comprehensive Report", style="bold yellow"))

        # Performance monitoring summary
        performance_summary = self.performance_monitor.get_summary()

        # Evaluate each experiment
        # Evaluate each experiment
        evaluation_results = {}
        for experiment_type, results in self.results.items():
            metrics = self.evaluator.generate_comprehensive_report(results)
            evaluation_results[experiment_type] = metrics

            # å¦‚æœæ˜¯ TaskBï¼Œåˆ™æ‰‹åŠ¨è½¬æ¢ä¸º dictï¼ˆå¦åˆ™ json.dump ä¼šæŠ¥é”™ï¼‰
            if experiment_type == "taskb":
                evaluation_results[experiment_type] = metrics.__dict__

        # Overall system metrics
        total_tasks = sum(
            results.get("total_tasks", 0) for results in self.results.values()
        )
        total_successful = sum(
            results.get("successful_tasks", 0) for results in self.results.values()
        )
        overall_success_rate = (
            total_successful / total_tasks if total_tasks > 0 else 0.0
        )

        comprehensive_report = {
            "experiment_summary": {
                "total_experiments": len(self.results),
                "total_tasks": total_tasks,
                "total_successful_tasks": total_successful,
                "overall_success_rate": overall_success_rate,
            },
            "performance_monitoring": performance_summary,
            "experiment_results": evaluation_results,
            "system_configuration": {
                "manager": self.config.get("manager", {"model": "N/A"}),
                "executors": [executor.executor_id for executor in getattr(self, "executors", [])],
            },
        }

        return comprehensive_report

    def display_results(self, report: Dict[str, Any]):
        """Display results in a formatted table."""
        console.print(Panel.fit("Experiment Results Summary", style="bold cyan"))

        # Create summary table
        table = Table(title="Experiment Results")
        table.add_column("Experiment Type", style="cyan", no_wrap=True)
        table.add_column("Tasks", justify="right", style="green")
        table.add_column("Success Rate", justify="right", style="green")
        table.add_column("Avg Time (s)", justify="right", style="yellow")
        table.add_column("Status", style="bold")

        for experiment_type, results in report["experiment_results"].items():
            table.add_row(
                experiment_type.replace("_", " ").title(),
                str(results["total_tasks"]),
                f"{results['success_rate']:.2%}",
                f"{results['average_execution_time']:.2f}",
                (
                    "âœ…"
                    if results.get("success_rate", 0.0) > 0.8
                    else "âš ï¸" if results["success_rate"] > 0.5 else "âŒ"
                ),
            )

        console.print(table)

        # Display overall metrics
        summary = report["experiment_summary"]
        console.print(f"\nğŸ“Š Overall Performance:")
        console.print(f"   â€¢ Total Tasks: {summary['total_tasks']}")
        console.print(f"   â€¢ Success Rate: {summary['overall_success_rate']:.2%}")
        console.print(f"   â€¢ Experiments Completed: {summary['total_experiments']}")

    def export_results(self, report: Dict[str, Any]):
        """Export results to files."""
        output_config = self.config["output"]
        results_dir = Path(output_config["results_dir"])
        results_dir.mkdir(exist_ok=True)

        if output_config["export_metrics"]:
            # Export comprehensive report
            report_file = results_dir / "comprehensive_report.json"
            with open(report_file, "w") as f:
                json.dump(report, f, indent=2, default=str)

            # Export individual experiment metrics
            for experiment_type, metrics in report["experiment_results"].items():
                metrics_file = results_dir / f"{experiment_type}_metrics.json"
                self.evaluator.export_metrics(metrics, str(metrics_file))

            console.print(f"ğŸ“ Results exported to {results_dir}")

    async def cleanup(self):
        """Cleanup resources and stop executors."""
        console.print("ğŸ§¹ Cleaning up resources...")

        try:
            # Stop all executors
            for executor in self.executors:
                await executor.stop()
                console.print(f"âœ… Stopped executor: {executor.executor_id}")

        except Exception as e:
            console.print(f"âš ï¸ Error during cleanup: {e}", style="yellow")

    async def run_pipeline(self):
        """Run the complete experimental pipeline."""
        try:
            console.print(Panel.fit("Starting Experiment Pipeline", style="bold green"))

            # Setup system
            await self.setup_system()

            # Run experiments based on configuration
            if self.config["experiments"]["summarization"]["enabled"]:
                self.results["summarization"] = (
                    await self.run_summarization_experiment()
                )

            if self.config["experiments"]["question_answering"]["enabled"]:
                self.results["question_answering"] = await self.run_qa_experiment()

            if self.config["experiments"]["table_generation"]["enabled"]:
                self.results["table_generation"] = (
                    await self.run_table_generation_experiment()
                )

            # Generate comprehensive report
            report = self.generate_comprehensive_report()

            # Display results
            self.display_results(report)

            # Export results
            self.export_results(report)

            console.print(
                Panel.fit("Pipeline Completed Successfully", style="bold green")
            )

        except Exception as e:
            console.print(f"âŒ Pipeline failed: {e}", style="bold red")
            logger.error(f"Pipeline error: {e}", exc_info=True)
            raise
        finally:
            # Always cleanup
            await self.cleanup()


@click.command()
@click.option("--config", "-c", "config_path", help="Path to configuration file")
@click.option(
    "--experiment",
    "-e",
    "experiment_type",
    type=click.Choice(["summarization", "qa", "table", "all", "taskb"]),
    default="taskb",
    help="Type of experiment to run",
)
@click.option("--manager-model", help="Manager LLM model name")
@click.option("--executor-model", help="Executor LLM model name")
@click.option("--output-dir", help="Output directory for results")

def main(config_path, experiment_type, manager_model, executor_model, output_dir):
    """Manager-Executor Collaboration System Pipeline."""

    console.print(
        Panel.fit(
            "Manager-Executor Collaboration System\n"
            "Hierarchical Multi-LLM Task Processing",
            style="bold blue",
        )
    )

    # Create pipeline
    pipeline = ExperimentPipeline(config_path)

    # Override config if command line options provided
    if manager_model:
        pipeline.config["manager"]["model"] = manager_model
    if executor_model:
        for executor_config in pipeline.config["executors"]:
            executor_config["model"] = executor_model
    if output_dir:
        pipeline.config["output"]["results_dir"] = output_dir

    # Run specific experiment or all
    if experiment_type == "all":
        asyncio.run(pipeline.run_pipeline())
    elif experiment_type == "taskb":
        # input_text = input("è¯·è¾“å…¥é—®é¢˜æè¿°ï¼š")
        # TODO: éœ€è¦æ”¹æˆä»æ–‡ä»¶è¯»å…¥ï¼Œæ•°æ®é›†å’ŒSummaryå®éªŒçš„æœ€å¥½ä¸€è‡´
        input_text="è¯·å¸®åŠ©æˆ‘åˆ†æä¸­å›½äººå£è€é¾„åŒ–å¸¦æ¥çš„ç¤¾ä¼šé—®é¢˜ï¼Œå¹¶æå‡ºåº”å¯¹ç­–ç•¥ã€‚"
        asyncio.run(pipeline.run_task_b_pipeline(input_text))
    else:
        # Run single experiment
        asyncio.run(pipeline.run_single_experiment(experiment_type))


if __name__ == "__main__":
    main()
